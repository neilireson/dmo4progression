{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#####################################################\n",
    "# Data preparation and exclusion\n",
    "#\n",
    "#####################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "data_dir = os.getenv(\"DATA_DIR\", \"Dataset/\")\n",
    "min_prev_score = 0\n",
    "threshold = 5.0\n",
    "min_progression = 0.5\n",
    "\n",
    "# read in main MS dataset\n",
    "data_file_path = data_dir + r\"MS_dataset_v.7.3.csv\"\n",
    "cols_from_dataset = ['Local.Participant', 'EDFSCR1L', 'visit.number', 'age', 'gender', 'mstypeonset', 'mstypediag', 'visit.date']\n",
    "data = pd.read_csv(data_file_path, usecols=cols_from_dataset)\n",
    "\n",
    "# forward fill within groups to ensure no leakage of demographics between participants\n",
    "cols_from_dataset = [col for col in cols_from_dataset if col in ['age', 'gender', 'mstypeonset', 'mstypediag']]\n",
    "for col in cols_from_dataset:\n",
    "    data[col] = data.groupby('Local.Participant')[col].ffill()\n",
    "\n",
    "# we need to get the time difference and convert from timedelta to something that the ML models can use - probably unix time\n",
    "data['time_since_diag'] = (pd.to_datetime(data['visit.date']) - pd.to_datetime(data['mstypediag'])).dt.total_seconds()\n",
    "data['time_since_onset'] = (pd.to_datetime(data['visit.date']) - pd.to_datetime(data['mstypeonset'])).dt.total_seconds()\n",
    "\n",
    "# drop cols used for calculation\n",
    "data = data.drop(['mstypeonset', 'mstypediag', 'visit.date'], axis=1)\n",
    "\n",
    "# convert gender to 0 and 1\n",
    "data['gender'] = data['gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# Crop down to just the necessary columns\n",
    "data = data.sort_values(by=['Local.Participant', 'visit.number'])\n",
    "\n",
    "# shift timepoints and EDSS to get previous as columns\n",
    "data['Prev_EDSS'] = data.groupby('Local.Participant')['EDFSCR1L'].shift(1)\n",
    "data['Prev_Timepoint'] = data.groupby('Local.Participant')['visit.number'].shift(1)\n",
    "\n",
    "# get progression label column\n",
    "progression_mask = (\n",
    "    (data['Prev_EDSS'] < threshold) &\n",
    "    (data['EDFSCR1L'] >= threshold) &\n",
    "    ((data['EDFSCR1L'] - data['Prev_EDSS']) >= min_progression)\n",
    ")\n",
    "data['progression'] = np.where(progression_mask, 1, 0)\n",
    "\n",
    "transitions = data[progression_mask]  # all the data where a transition happens, just in case.\n",
    "\n",
    "data['Prev_Timepoint'] = data['Prev_Timepoint'].str.upper()  # needed for later merges\n",
    "data['visit.number'] = data['visit.number'].str.upper()  # this one is just for consistency\n",
    "\n",
    "#display(transitions.head(10))\n",
    "\n",
    "# now that we have a reduced dataset, loop through our weekly dmo aggregations and append the relevant ones to the dataframe\n",
    "timepoints = [\"T1\", \"T2\", \"T3\", \"T4\"]  # we don't need DMOs where T5 is previous\n",
    "\n",
    "dfs_list = []\n",
    "for timepoint in timepoints:\n",
    "    feature_file_path = data_dir + f\"/{timepoint} Aggregated DMO Data_V7.3/cvs-{timepoint}-weekly_agg_all-21-01-2026.csv\"\n",
    "    temp_df = pd.read_csv(feature_file_path)\n",
    "    dfs_list.append(temp_df)\n",
    "\n",
    "features_df = pd.concat(dfs_list, ignore_index=True)\n",
    "data = pd.merge(data, features_df, left_on=['Local.Participant', 'Prev_Timepoint'],\n",
    "                right_on=['participant_id', 'visit_type'], how='left')\n",
    "\n",
    "data = data.drop(['participant_id', 'visit_type'], axis=1)\n",
    "data = data.dropna(subset=['Prev_Timepoint'])\n",
    "\n",
    "# Data cleaning - will monitor number of rows throughout\n",
    "print(f\"Initially {len(data)} rows.\")\n",
    "\n",
    "# get rid of participants where either current or previous EDSS is missing\n",
    "count_before = len(data)\n",
    "data = data.dropna(subset=['EDFSCR1L', 'Prev_EDSS'])\n",
    "print(f\"{count_before - len(data)} rows have missing EDSS values and will be dropped...\")\n",
    "\n",
    "# Create matrix of prev -> next EDSS scores\n",
    "prev_scores = data['Prev_EDSS'].to_numpy().tolist()\n",
    "next_scores = data['EDFSCR1L'].to_numpy().tolist()\n",
    "\n",
    "# extract the unique scores\n",
    "scores = list(set(prev_scores).union(set(next_scores)))\n",
    "scores.sort()\n",
    "n_scores = len(scores)\n",
    "\n",
    "# Create transition matrix\n",
    "# Note some of these transitions will be for people with missing DMOs\n",
    "transition_matrix = pd.DataFrame(\n",
    "    np.zeros((n_scores, n_scores), dtype=int),\n",
    "    index=scores,\n",
    "    columns=scores)\n",
    "\n",
    "for prev_score, next_score in zip(prev_scores, next_scores):\n",
    "    transition_matrix.loc[prev_score, next_score] += 1\n",
    "\n",
    "# Pretty print transition matrix as heatmap\n",
    "plt.subplots(figsize=(10, 8))\n",
    "ax = plt.subplot(111)\n",
    "sns.heatmap(transition_matrix.values, annot=True, cmap=\"Blues\", ax=ax, fmt='d', norm=colors.LogNorm())\n",
    "ax.set_xlabel('Next Score')\n",
    "ax.set_ylabel('Previous Score')\n",
    "ax.xaxis.set_ticklabels(scores)\n",
    "ax.yaxis.set_ticklabels(scores)\n",
    "plt.show()\n",
    "\n",
    "# Exclude participants with Prev_EDSS >= threshold as they have already progressed:\n",
    "count_before = len(data)\n",
    "high_prev_edss_mask = (data['Prev_EDSS'] >= threshold)\n",
    "print(f\"{sum(high_prev_edss_mask)} rows have a previous EDSS >= {threshold} and will be dropped...\")\n",
    "data = data.drop(data[high_prev_edss_mask].index)\n",
    "\n",
    "# Exclude participants with Prev_EDSS < min_prev_score as they are highly unlikely to progress, and may have different mobility patterns\n",
    "count_before = len(data)\n",
    "low_prev_edss_mask = (data['Prev_EDSS'] < min_prev_score)\n",
    "print(f\"{sum(low_prev_edss_mask)} rows have a previous EDSS < {min_prev_score} and will be dropped...\")\n",
    "data = data.drop(data[low_prev_edss_mask].index)\n",
    "\n",
    "# drop all remaining NAN rows, which will be due to missing DMOs\n",
    "count_before = len(data)\n",
    "data = data.dropna()\n",
    "print(f\"{count_before - len(data)} rows have missing DMOs and will be dropped...\")\n",
    "\n",
    "num_progressions = len(data[data[\"progression\"] == 1])\n",
    "unique_sub_progressions = len(data[data[\"progression\"] == 1]['Local.Participant'].unique())\n",
    "\n",
    "print(\n",
    "    f\"We have {num_progressions} progressions across {unique_sub_progressions} subjects, out of a total of {len(data)} valid samples.\")\n",
    "\n",
    "display(data[data[\"progression\"] == 1].head(5))\n",
    "\n",
    "#labels_df.to_csv(\"data.csv\", index=False)\n",
    "\n",
    "# need to be at least 1 (not 0.5) to change\n",
    "# subset of EDSS and normalise?\n",
    "# balance dataset - Done very good\n",
    "\n",
    "# undersampled_df = labels_df.groupby('progression').sample(labels_df.groupby('progression').size().min())"
   ],
   "id": "811b37a2f6d0dae0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###################################################################\n",
    "# Feature selection and exclusion\n",
    "# Note that in data preparation rows with NaN values are excluded.\n",
    "# Removing these features (columns) may increase the data,\n",
    "# if that feature is the reason for NaN exclusion.\n",
    "# ToDo Test with various thresholds\n",
    "###################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mi_threshold = 0.005\n",
    "feature_selection_type = None\n",
    "\n",
    "\n",
    "def rf_feature_analysis(X, y, random_state=42):\n",
    "    model = RandomForestRegressor(random_state=random_state)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # The importance of a feature is how much this feature is used in each tree of the forest.\n",
    "    # It is computed as the (normalized) total reduction of the criterion brought by that feature.\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    importances = pd.Series(model.feature_importances_, index=np.array(X.columns))\n",
    "    importances = pd.Series(data=importances, index=X.columns).sort_values(ascending=False)\n",
    "    importances.plot.bar(yerr=std, ax=ax)\n",
    "    ax.set_title(\"Feature importance using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "def rf_perm_feature_analysis(X, y, random_state=42):\n",
    "    model = RandomForestClassifier(random_state=random_state)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    result = permutation_importance(model, X, y, n_repeats=10, random_state=random_state, n_jobs=2)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    importances = pd.Series(result.importances_mean, index=np.array(X.columns))\n",
    "    importances = pd.Series(data=importances, index=X.columns).sort_values(ascending=False)\n",
    "    importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "    ax.set_title(\"Feature importance using permutation on full model\")\n",
    "    ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_permutation_importance(clf, X, y, ax, indexes=None, random_state=42):\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=random_state, n_jobs=2)\n",
    "    if indexes is not None:\n",
    "        perm_sorted_idx = indexes\n",
    "    else:\n",
    "        perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    tick_labels_dict = {\"tick_labels\": X.columns[perm_sorted_idx]}\n",
    "    ax.boxplot(result.importances[perm_sorted_idx].T, vert=False, **tick_labels_dict)\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def rf_combine_feature_analysis(X, y, random_state=42):\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    mdi_importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "    tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    mdi_importances.sort_values().plot.barh(ax=ax1)\n",
    "    ax1.set_xlabel(\"Gini importance\")\n",
    "    plot_permutation_importance(clf, X, y, ax2, tree_importance_sorted_idx)\n",
    "    ax2.set_xlabel(\"Decrease in accuracy score\")\n",
    "    fig.suptitle(\n",
    "        \"Impurity-based vs. permutation importance on multicollinear features (train set)\"\n",
    "    )\n",
    "    _ = fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mi_feature_analysis(X, y, mi_threshold=0.0, random_state=42):\n",
    "    # feature importance analysis\n",
    "    pearson_corr = X.corrwith(y).sort_values(ascending=False)\n",
    "    display(pearson_corr)\n",
    "\n",
    "    # mutual information gain for feature removal\n",
    "    mi = mutual_info_classif(X, y, random_state=random_state, n_jobs=-1)\n",
    "    mi = pd.Series(data=mi, index=X.columns).sort_values(ascending=False)\n",
    "    mi.plot(kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "    # delete features with 0 information gain\n",
    "    mi = mi[mi > mi_threshold]\n",
    "\n",
    "    X = X[mi.index]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def show_correlation(X):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    corr = spearmanr(X).correlation\n",
    "\n",
    "    # Ensure the correlation matrix is symmetric\n",
    "    corr = (corr + corr.T) / 2\n",
    "    np.fill_diagonal(corr, 1)\n",
    "\n",
    "    # We convert the correlation matrix to a distance matrix before performing\n",
    "    # hierarchical clustering using Ward's linkage.\n",
    "    distance_matrix = 1 - np.abs(corr)\n",
    "    dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "    dendro = hierarchy.dendrogram(\n",
    "        dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90\n",
    "    )\n",
    "    dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "    ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "    ax2.set_xticks(dendro_idx)\n",
    "    ax2.set_yticks(dendro_idx)\n",
    "    ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "    ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "    _ = fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dist_linkage\n",
    "\n",
    "\n",
    "def cluster_selection(X, y, dist_linkage, threshold=0.15, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=0.5)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Baseline accuracy on test data: {clf.score(X_test, y_test):.2}\")\n",
    "\n",
    "    cluster_ids = hierarchy.fcluster(dist_linkage, threshold, criterion=\"distance\")\n",
    "    cluster_id_to_feature_ids = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(cluster_ids):\n",
    "        cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "    selected_features_names = X.columns[selected_features]\n",
    "\n",
    "    X_train_sel = X_train[selected_features_names]\n",
    "    X_test_sel = X_test[selected_features_names]\n",
    "\n",
    "    clf_sel = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    clf_sel.fit(X_train_sel, y_train)\n",
    "    print(\n",
    "        \"Baseline accuracy on test data with features removed:\"\n",
    "        f\" {clf_sel.score(X_test_sel, y_test):.2}\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    plot_permutation_importance(clf_sel, X_test_sel, y_test, ax)\n",
    "    ax.set_title(\"Permutation Importance on selected subset of features\\n(test set)\")\n",
    "    ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "    ax.figure.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return X[selected_features_names]\n",
    "\n",
    "\n",
    "def pca_selection(X, random_state=42):\n",
    "    # normalisation\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=0.95, svd_solver='full', random_state=random_state)  # keep 95% of variance\n",
    "    X_pca = pd.DataFrame(data=pca.fit_transform(X_scaled))\n",
    "    print(\"Explained variance:\", pca.explained_variance_ratio_)\n",
    "    print(\"Cumulative:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "    print(\"Shape of Original Dataset:\", X_scaled.shape)\n",
    "    print(\"Shape after PCA:\", X_pca.shape)\n",
    "\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(loadings, annot=True, cmap='coolwarm', yticklabels=np.array(X.columns))\n",
    "    plt.title('Feature Importance in Principal Components')\n",
    "    plt.show()\n",
    "\n",
    "    return X_pca\n",
    "\n",
    "\n",
    "# drop irrelevant or 'cheating' columns\n",
    "y_data = data[\"progression\"].reset_index(drop=True)  # can be data[\"progression\"] or data[\"multiclass_progression\"]\n",
    "labels_X_all = data.reset_index(drop=True)\n",
    "X_data = data.drop([\"progression\", \"Local.Participant\", \"EDFSCR1L\",\n",
    "                    \"visit.number\", \"Prev_Timepoint\"], axis=1)\n",
    "\n",
    "# remove any feature with an MI pf 0\n",
    "X_data = mi_feature_analysis(X_data, y_data, mi_threshold=0)\n",
    "print(\"Removed irrelevant (i.e. MI == 0) features\")\n",
    "\n",
    "# rf_feature_analysis(X_data, y_data)\n",
    "# rf_perm_feature_analysis(X_data, y_data)\n",
    "rf_combine_feature_analysis(X_data, y_data)\n",
    "\n",
    "dist_linkage = show_correlation(X_data)\n",
    "X_data_cluster = cluster_selection(X_data, y_data, dist_linkage)\n",
    "\n",
    "X_data_pca = pca_selection(X_data)\n",
    "\n",
    "if (feature_selection_type == \"cluster\"):\n",
    "    X_data = X_data_cluster\n",
    "elif (feature_selection_type == \"mi\"):\n",
    "    if (mi_threshold > 0):\n",
    "        X_data = mi_feature_analysis(X_data, y_data, mi_threshold=mi_threshold)\n",
    "elif (feature_selection_type == \"pca\"):\n",
    "    X_data = X_data_pca\n",
    "elif (feature_selection_type is not None):\n",
    "    print(\"Feature selection type not recognized\")\n",
    "\n",
    "print(\"Feature selection:\", feature_selection_type)\n",
    "print(\"X_data shape:\", X_data.shape)"
   ],
   "id": "ba699ef386e451dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "######################################################################\n",
    "# Resampling data (NOT USED)\n",
    "# Note this needs to be tidied up into the experimentation framework\n",
    "######################################################################\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "do_data_sampling = False\n",
    "\n",
    "\n",
    "def get_unsampled(x_data, x_sampled, y_data):\n",
    "    # Create dataframe with column showing whether data has been sampled or not\n",
    "    # Shown by column \"_merge\" value \"both\" == sampled \"left_only\" == not sampled\n",
    "    merged = x_data.merge(x_sampled.drop_duplicates(), how='left', indicator=True)\n",
    "    # Get indices of rows which have not been sampled\n",
    "    unsampled_indices = merged.index[merged['_merge'] == \"left_only\"].tolist()\n",
    "    X_unsampled = X_data.iloc[unsampled_indices]\n",
    "    y_unsampled = y_data.iloc[unsampled_indices]\n",
    "    return X_unsampled, y_unsampled\n",
    "\n",
    "\n",
    "if (do_data_sampling):\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "\n",
    "    X_data, y_data = smote.fit_resample(X_data, y_data)\n"
   ],
   "id": "99ef1cbcf31c130f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "n_iter=20\n",
    "random_state=42\n",
    "verbose = 0\n",
    "n_jobs=-1\n",
    "cv=5\n",
    "scoring='f1_macro'\n",
    "\n",
    "svm_model = BayesSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),  # Regularization parameter\n",
    "        'gamma': Real(1e-6, 1e+1, prior='log-uniform'),  # Kernel coefficient\n",
    "        'kernel': Categorical(['rbf', 'poly', 'sigmoid']),  # Type of kernel\n",
    "        'degree': Integer(1, 5)  # Degree for poly kernel\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "mlp_model = BayesSearchCV(\n",
    "    MLPClassifier(random_state=random_state, max_iter=1000),\n",
    "    {\n",
    "        'hidden_layer_sizes': Integer(1, 128),  # Different architectures\n",
    "        'activation': Categorical(['tanh', 'relu']),\n",
    "        'solver': Categorical(['sgd', 'adam']),\n",
    "        'alpha': Real(1e-5, 1e-1, prior='log-uniform'),  # L2 penalty (regularization)\n",
    "        'learning_rate': Categorical(['constant', 'adaptive']),\n",
    "        'learning_rate_init': Real(1e-4, 1e-1, prior='log-uniform')  # Initial learning rate\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "knn_model = BayesSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    {\n",
    "        'n_neighbors': Integer(1, 50),\n",
    "        'weights': Categorical(['uniform', 'distance']),\n",
    "        'algorithm': Categorical(['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size': Integer(10, 50),\n",
    "        'p': Integer(1, 2)  # 1 = Manhattan, 2 = Euclidean distance\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "xgb_model = BayesSearchCV(\n",
    "    XGBClassifier(random_state=random_state, use_label_encoder=False, eval_metric='logloss'),\n",
    "    {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'learning_rate': Real(0.01, 1.0, prior='log-uniform'),\n",
    "        'subsample': Real(0.5, 1.0, prior='uniform'),  # Fraction of samples used for fitting\n",
    "        'colsample_bytree': Real(0.5, 1.0, prior='uniform'),  # Fraction of features used per tree\n",
    "        'gamma': Real(1e-6, 5, prior='log-uniform'),  # Min loss reduction for split\n",
    "        'reg_alpha': Real(1e-5, 10, prior='log-uniform'),  # L1 regularization\n",
    "        'reg_lambda': Real(1e-5, 10, prior='log-uniform')  # L2 regularization\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "rf_model = BayesSearchCV(\n",
    "    RandomForestClassifier(random_state=random_state),\n",
    "    {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'max_depth': Integer(5, 50),\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),  # Number of features to consider at each split\n",
    "        'bootstrap': Categorical([True, False]),\n",
    "        'criterion': Categorical(['gini', 'entropy'])\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "dt_model = BayesSearchCV(\n",
    "    DecisionTreeClassifier(random_state=random_state),\n",
    "    {\n",
    "        'criterion': Categorical(['gini', 'entropy']),\n",
    "        'max_depth': Integer(3, 30),\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'min_samples_leaf': Integer(1, 20),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "        'ccp_alpha': Real(0.0, 0.1, prior='uniform')  # Cost-Complexity Pruning alpha\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "lr_model = BayesSearchCV(\n",
    "    LogisticRegression(random_state=random_state, solver='saga', max_iter=5000),\n",
    "    {\n",
    "        'C': Real(1e-6, 1e+6, prior='log-uniform'),  # Inverse of regularization strength\n",
    "        'penalty': Categorical(['l1', 'l2', 'elasticnet']),\n",
    "        'l1_ratio': Real(0, 1, prior='uniform'),  # Only used if penalty='elasticnet'\n",
    "        'fit_intercept': Categorical([True, False])\n",
    "    },\n",
    "    n_iter=n_iter,\n",
    "    random_state=random_state,\n",
    "    verbose=verbose,\n",
    "    n_jobs=n_jobs,\n",
    "    cv=cv,\n",
    "    scoring=scoring\n",
    ")"
   ],
   "id": "580217db59c1d1e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "######################################################################\n",
    "# TEST Classification / Prediction\n",
    "# Note this needs to be tidied up into the experimentation framework\n",
    "######################################################################\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "    confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import faulthandler\n",
    "#import signal\n",
    "\n",
    "#faulthandler.register(signal.SIGINT.value) # I'm not sure what this was doing but it wasn't working for me\n",
    "\n",
    "# Data normalisation (Not the impact of this needs to be tested)\n",
    "# e.g. StandardScaler, MinMaxScaler, RobustScaler\n",
    "# If columns have different (e.g. skewed) distributions that may need different normalisations\n",
    "# e.g. Log normalisation\n",
    "# Note that StandardScaler combined with small data will hang SVC, MinMaxScaler works\n",
    "scaler = MinMaxScaler()\n",
    "cv_folds = 5\n",
    "\n",
    "\n",
    "def do_cv(model, X_data, y_data, n_splits=cv_folds):\n",
    "    # requires: set_config(transform_output=\"pandas\")\n",
    "    # transformer = ColumnTransformer([('standard_scaler', ss, feature_cols)],\n",
    "    #                                 remainder='passthrough',\n",
    "    #                                 verbose_feature_names_out=False)\n",
    "\n",
    "    combined_cm = None\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for i, (train_split_indices, test_split_indices) in enumerate(kf.split(X_data)):\n",
    "\n",
    "        X_train_split = X_data.iloc[train_split_indices]\n",
    "        X_test_split = X_data.iloc[test_split_indices]\n",
    "        y_train_split = y_data.iloc[train_split_indices]\n",
    "        y_test_split = y_data.iloc[test_split_indices]\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "        X_test_scaled = scaler.transform(X_test_split)\n",
    "\n",
    "        # sample weighting\n",
    "        sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_split)\n",
    "\n",
    "        try:\n",
    "            model.fit(X_train_scaled, y_train_split, sample_weight=sample_weights)\n",
    "        except:\n",
    "            print(\"Sample weighting not accepted, training without...\")\n",
    "            model.fit(X_train_scaled, y_train_split)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_test_split, y_pred):.2f}\")\n",
    "        print(f\"Precision: {precision_score(y_test_split, y_pred, average='macro', zero_division=0):.2f}\")\n",
    "        print(f\"Recall: {recall_score(y_test_split, y_pred, average='macro', zero_division=0):.2f}\")\n",
    "        print(f\"F1 Score: {f1_score(y_test_split, y_pred, average='macro', zero_division=0):.2f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_test_split, y_pred)\n",
    "        if combined_cm is None:\n",
    "            combined_cm = cm\n",
    "        else:\n",
    "            combined_cm = combined_cm + cm\n",
    "\n",
    "    return combined_cm\n",
    "\n",
    "\n",
    "def show_evaluation(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "    plt.title('Confusion Matrix: Predicted vs. Actual')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# sample weighting\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "model = svm_model\n",
    "\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "# Print the best parameters\n",
    "print(f\"Best Parameters: {model.best_params_}\")\n",
    "print(f\"Best Score: {model.best_score_}\")\n",
    "model = model.best_estimator_  # gets the best estimator so we're not doing bayesian searching again on each fold\n",
    "show_evaluation(model, X_test, y_test)\n",
    "\n",
    "# Annoyingly cross_val_score and cross_val_predict can differ ???\n",
    "conf_mat = do_cv(model, X_data, y_data)\n",
    "\n",
    "# Pretty print confusion matrix\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "cm = pd.DataFrame(conf_mat, range(2), range(2))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", ax=ax, fmt='d')\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.xaxis.set_ticklabels(['0', '1'])\n",
    "ax.yaxis.set_ticklabels(['0', '1'])\n",
    "plt.show()"
   ],
   "id": "a7fea54ee2f04c48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def fit_and_evaluate(model, model_name, X_train, X_test, y_train, y_test, X_resampled, y_resampled):\n",
    "    print(f\"\\n{'=' * 40}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'=' * 40}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Best CV Score: {model.best_score_:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    return model.best_estimator_\n",
    "\n",
    "\n",
    "def get_metrics_from_cm(cm):\n",
    "    tn, fp, fn, tp = cm.values.ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_data, y_data):\n",
    "    conf_mat = do_cv(model, X_data, y_data)\n",
    "    cm = pd.DataFrame(conf_mat, range(2), range(2))\n",
    "    metrics = get_metrics_from_cm(cm)\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", ax=ax, fmt='d')\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.xaxis.set_ticklabels(['0', '1'])\n",
    "    ax.yaxis.set_ticklabels(['0', '1'])\n",
    "    plt.show()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# all models in one dict with names\n",
    "models_dict = {\n",
    "    \"SVM\": svm_model,\n",
    "    \"MLP\": mlp_model,\n",
    "    \"KNN\": knn_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Decision Tree\": dt_model,\n",
    "    \"Logistic Regression\": lr_model\n",
    "}\n",
    "\n",
    "model_performance = []\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    best_model = fit_and_evaluate(model, name, X_train, X_test, y_train, y_test)\n",
    "    metrics = evaluate_model(best_model, X_data, y_data)\n",
    "\n",
    "    metrics['Model'] = name\n",
    "    model_performance.append(metrics)\n",
    "\n",
    "performance_df = pd.DataFrame(model_performance)\n",
    "cols = ['Model'] + [c for c in performance_df.columns if c != 'Model']\n",
    "performance_df = performance_df[cols]\n",
    "performance_df.set_index('Model', inplace=True)\n",
    "performance_df = performance_df.sort_values(by='F1 Score', ascending=False)\n",
    "\n",
    "display(performance_df)"
   ],
   "id": "c994814f3c4ee5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# un-normalise the data and try to find out which samples were misclassified\n",
    "train_predictions = best_model.predict(X_train)\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "indexed_train_predictions = y_train.reset_index()\n",
    "indexed_train_predictions[\"Prediction\"] = train_predictions\n",
    "indexed_train_predictions.index = indexed_train_predictions[\"index\"]\n",
    "indexed_train_predictions = indexed_train_predictions.drop([\"index\"], axis=1)\n",
    "\n",
    "indexed_test_predictions = y_test.reset_index()\n",
    "indexed_test_predictions[\"Prediction\"] = test_predictions\n",
    "indexed_test_predictions.index = indexed_test_predictions[\"index\"]\n",
    "indexed_test_predictions = indexed_test_predictions.drop([\"index\"], axis=1)\n",
    "\n",
    "predictions_to_be_appended = pd.concat([indexed_train_predictions, indexed_test_predictions], axis=0)\n",
    "predictions_to_be_appended[\"misclassified\"] = abs(\n",
    "    predictions_to_be_appended[\"Prediction\"] - predictions_to_be_appended[\"progression\"])\n",
    "predictions_to_be_appended = predictions_to_be_appended.drop([\"progression\"], axis=1)\n",
    "\n",
    "original_data_with_predictions = labels_X_all.join(predictions_to_be_appended)\n",
    "\n",
    "display(original_data_with_predictions[original_data_with_predictions[\"misclassified\"] == 1])"
   ],
   "id": "97f5a7e749693a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "\n",
    "prog_group = original_data_with_predictions[original_data_with_predictions[\"progression\"] == 1]\n",
    "no_prog_group = original_data_with_predictions[original_data_with_predictions[\"progression\"] == 0]\n",
    "\n",
    "feature_cols = [c for c in original_data_with_predictions.columns if c != \"progression\"]\n",
    "\n",
    "for col in feature_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(original_data_with_predictions[col]):\n",
    "\n",
    "        sns.kdeplot(data=no_prog_group, x=col, fill=True,\n",
    "                    color='forestgreen', label='No Progression', alpha=0.3, linewidth=2)\n",
    "        sns.kdeplot(data=prog_group, x=col, fill=True,\n",
    "                    color='crimson', label='Progression', alpha=0.4, linewidth=2)\n",
    "\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "\n",
    "    else:\n",
    "        temp_df = original_data_with_predictions.groupby('progression')[col].value_counts(normalize=True).rename(\n",
    "            'proportion').reset_index()\n",
    "        temp_df['progression_label'] = temp_df['progression'].map({0: 'No Progression', 1: 'Progression'})\n",
    "\n",
    "        sns.barplot(data=temp_df, x=col, y='proportion', hue='progression_label',\n",
    "                    palette={'No Progression': 'forestgreen', 'Progression': 'crimson'}, edgecolor='black')\n",
    "\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.legend(title=None)\n",
    "\n",
    "    plt.title(f'{col}')\n",
    "    plt.xlabel(col)\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "8b854e39e4028749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82afbe676c3bf3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f4e55bf0d6c51cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8ede1a5cf474ceb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
